#cahierDeCharge.py
import streamlit as st
from langchain.prompts.chat import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate
from langchain.schema import SystemMessage
from prompts import initial_questions, section_prompts , summary_sections , system_prompt , system_summary_prompt




def handle_token_limit_error_in_section(historique_container):
    """
    G√®re les cas o√π la limite de tokens est atteinte dans une section.
    Affiche un message clair et ajoute un bouton pour aller directement √† la derni√®re √©tape.
    """
    # Affiche un message d'erreur expliquant la situation
    historique_container.error(
        """
        ‚ùå **Limite atteinte : Donn√©es trop volumineuses**  
        L'IA a atteint sa limite de traitement pour cette section.  
        üëâ **Veuillez continuer avec la section suivante** pour √©viter de perdre des donn√©es.
        
        ‚ö†Ô∏è Note : Certaines parties de cette section pourraient ne pas √™tre incluses dans le r√©sum√© final.
        """,
        icon="‚ö†Ô∏è",
    )

    # Bouton pour aller directement √† la derni√®re √©tape
    if historique_container.button(
        "üìÑ Aller √† la derni√®re √©tape : G√©n√©ration du cahier des charges", type="primary"
    ):
        # Mettre √† jour la section actuelle
        st.session_state.current_section = "G√©n√©ration de Cahier des Charges"

        # Ajouter un message de confirmation √† l'historique
        st.session_state.chat_history.append({
            'human': None,
            'AI': """
            Vous avez choisi de passer directement √† la derni√®re √©tape.  
            Vous pouvez maintenant g√©n√©rer le cahier des charges final.
            """
        })


         # Debug : v√©rifier l'historique
    st.write("√âtat de l'historique apr√®s l'ajout :", st.session_state.chat_history)
##############################################################################
#                       FONCTIONS DE G√âN√âRATION DE PROMPTS                   # 
##############################################################################
def generate_full_prompt(current_section, previous_summaries):
    """
    G√©n√®re le prompt complet pour l'IA en combinant :
    - Le prompt g√©n√©ral (system_prompt) : cadre global.
    - Le r√©sum√© des sections pr√©c√©dentes : contexte.
    - Le prompt sp√©cifique √† la section actuelle : objectif.

    Args:
        current_section (str): Nom de la section actuelle.
        previous_summaries (str): R√©sum√© des sections pr√©c√©dentes.

    Returns:
        str: Prompt complet pour l'IA.
    """
    # R√©cup√©rer le prompt sp√©cifique √† la section actuelle
    section_prompt = section_prompts.get(current_section, "")
    
    # Combiner les diff√©rents √©l√©ments pour former le prompt complet
    full_prompt = f"""
    {system_prompt}
    
    ### R√©sum√© des sections pr√©c√©dentes :
    {previous_summaries}
    
    ### Section actuelle : {current_section}
    {section_prompt}
    """
    return full_prompt

def generate_previous_summaries(completed_sections):
    """
    Combine tous les r√©sum√©s d√©j√† g√©n√©r√©s pour les sections compl√©t√©es.
    Utilise les r√©sum√©s r√©els stock√©s dans st.session_state.history_summary.
    """
    # Initialisation d‚Äôune liste pour stocker les r√©sum√©s
    summaries = []
    
    # Parcourir les sections compl√©t√©es
    for section in completed_sections:
        # Chercher le r√©sum√© correspondant dans l'historique
        summary = next(
            (entry['summary'] for entry in st.session_state.history_summary if entry['section'] == section),
            "(R√©sum√© non disponible)"  # Par d√©faut si aucun r√©sum√© trouv√©
        )
        # Ajouter le r√©sum√© format√© √† la liste
        summaries.append(f"R√©sum√© pour {section} : {summary}")
    
    # Combiner tous les r√©sum√©s en un seul texte
    return "\n".join(summaries)


def generate_summary_prompt(system_summary_prompt, previous_summaries, section_name, summary_prompt):
    """
    Combine le system_summary_prompt, les r√©sum√©s pr√©c√©dents, 
    et le prompt sp√©cifique √† la section actuelle pour cr√©er un prompt complet.
    """
    return f"""
    {system_summary_prompt}
    
    ### R√©sum√© des sections pr√©c√©dentes :
    {previous_summaries}
    
    ### Section actuelle : {section_name}
    {summary_prompt}
    """
    

    
##############################################################################
#                     FONCTIONS DE NAVIGATION ENTRE SECTIONS                 #
##############################################################################
def generate_summary_document():
    """G√©n√®re un document combinant tous les r√©sum√©s en un seul fichier texte sans redondance."""
    summary_data = []

    for entry in st.session_state.history_summary:
        # Ajouter uniquement le contenu du r√©sum√© sans r√©p√©ter le titre de la section
        summary_data.append(entry['summary'])
    
    return "\n\n".join(summary_data)

def next_section():
    """Passe √† la section suivante ou propose de t√©l√©charger le r√©sum√© √† la fin."""
    sections = list(section_prompts.keys())
    current_index = sections.index(st.session_state.current_section)

    # G√©n√©rer le r√©sum√© pour la section actuelle
    generate_summary()

    if current_index < len(sections) - 1:
        # Passer √† la section suivante
        st.session_state.current_section = sections[current_index + 1]

        # Ajouter le titre de la section √† l'historique
        st.session_state.chat_history.append({
            'human': None,
            'AI': f"### {st.session_state.current_section}"
        })

        # Mettre √† jour les prompts
        previous_summaries = generate_previous_summaries(sections[:current_index + 1])
        section_name = st.session_state.current_section
        summary_prompt = summary_sections.get(section_name, "Aucun prompt sp√©cifique pour cette section.")
        st.session_state.full_summary_prompt = generate_summary_prompt(
            system_summary_prompt, 
            previous_summaries, 
            section_name, 
            summary_prompt
        )        
        st.session_state.full_prompt = generate_full_prompt(
            st.session_state.current_section, 
            previous_summaries
        )

        # Ajouter la question initiale de la section
        initial_question = initial_questions.get(section_name, "")
        if initial_question:
            st.session_state.chat_history.append({"human": "", "AI": initial_question})
            # Ajouter le message de fin et le bouton de t√©l√©chargement dans l'historique
        if st.session_state.current_section == "G√©n√©ration de Cahier des Charges":

            final_message = """
            F√©licitations üéâ ! Vous avez termin√© toutes les sections.  
            Vous pouvez maintenant t√©l√©charger le r√©sum√© complet en appuyant sur le bouton ci-dessous.
            """
            st.session_state.chat_history.append({
                'human': None,
                'AI': final_message
            })
            st.session_state.chat_history.append({
                'human': None,
                'AI': 'üì• [Cliquez ici pour t√©l√©charger le r√©sum√©](download_link)'
            })  
    else:
         st.session_state.chat_history.append({
            'human': None,
            'AI': """
            Merci pour votre confiance et d'avoir choisi TEKIN. Vous √™tes d√©j√† √† la fin du processus.
            Si vous avez d'autres questions, n'h√©sitez pas √† les poser ! üòä
            """
        })
        
##############################################################################
#                     FONCTIONS DE GESTION DES R√âSUM√âS                       #
##############################################################################

def generate_summary():
    """G√©n√®re un r√©sum√© pour la section actuelle et l'ajoute √† l'historique des r√©sum√©s."""
    try:
        # Ignorer la section Accueil
        if st.session_state.current_section == "Accueil":
            return

        # V√©rifier si 'groq_chat' est bien initialis√©
        if 'groq_chat' not in st.session_state:
            st.error("groq_chat n'est pas initialis√©. Assurez-vous que la configuration est correcte.")
            return

        # V√©rifier que l'historique des conversations est non vide
        if not st.session_state.chat_history:
            st.warning("Aucune conversation disponible pour g√©n√©rer un r√©sum√©.")
            return

        # Transformer l'historique des conversations en format compatible LangChain
        formatted_history = [
            {'role': 'user', 'content': msg['human']} if idx % 2 == 0 else {'role': 'assistant', 'content': msg['AI']}
            for idx, msg in enumerate(st.session_state.chat_history)
        ]

        # Ajouter une consigne explicite pour forcer la structure
        section_prompt = summary_sections.get(st.session_state.current_section, "")
        structured_prompt = f"""
        R√©sume la section '{st.session_state.current_section}' en respectant strictement la structure suivante :
        
        {section_prompt}
        """

        # Construire le prompt
        summary_prompt = prompt_summary.format_prompt(
            chat_history=formatted_history,
            human_input=structured_prompt
        ).to_messages()

        # Appeler le mod√®le pour g√©n√©rer le r√©sum√©
        response = st.session_state.groq_chat(summary_prompt)

        # Ajouter le r√©sum√© g√©n√©r√© √† l'historique des r√©sum√©s
        st.session_state.history_summary.append({
            'section': st.session_state.current_section,
            'summary': response.content
        })
    except Exception as e:
        st.error(f"Erreur lors de la g√©n√©ration du r√©sum√© : {str(e)}")

def display_summary_history():
    """Affiche l'historique des r√©sum√©s dans l'interface Streamlit."""
    st.subheader("Historique des R√©sum√©s")
    if st.session_state.history_summary:
        for entry in st.session_state.history_summary:
            with st.expander(f"R√©sum√© pour la section : {entry['section']}"):
                st.text_area("R√©sum√© :", entry['summary'], height=150, key=f"summary_{entry['section']}")
    else:
        st.info("Aucun r√©sum√© disponible.")


##############################################################################
#                     5. FONCTIONS DE GESTION DES TEMPLATES DE PROMPTS       #
##############################################################################

def get_updated_internal_summary_prompt_template():
    """Retourne un template de prompt mis √† jour pour les r√©sum√©s internes."""
    full_summary_prompt = st.session_state.get("full_summary_prompt", "Default summary prompt")
    return ChatPromptTemplate.from_messages([
        SystemMessage(content=full_summary_prompt),
        MessagesPlaceholder(variable_name="history_summary"),
        HumanMessagePromptTemplate.from_template("{human_input}")
    ])

# Fonction pour g√©n√©rer le Chat Prompt Template
def get_updated_prompt_template():
    """Retourne le prompt_template mis √† jour avec le full prompt actuel."""
    full_prompt = st.session_state.get("full_prompt", "Default system prompt")
    return ChatPromptTemplate.from_messages([
        SystemMessage(content=full_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        HumanMessagePromptTemplate.from_template("{human_input}")
    ])

##############################################################################
#                            6. INITIALISATION                              #
##############################################################################
def init():
    """Initialise les variables globales n√©cessaires."""

    # Initialiser le full_summary_prompt avec system_summary_prompt
    st.session_state.full_summary_prompt = system_summary_prompt
    global prompt_summary
    prompt_summary = ChatPromptTemplate.from_messages([
        SystemMessage(content=st.session_state.full_summary_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        HumanMessagePromptTemplate.from_template("{human_input}")
    ])

